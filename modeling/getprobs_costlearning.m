function [map,negllh,llh] = getprobs_costlearning(params)

%Get probability values for specific parameter values of cost updating
%model (alpha and delta update rules, all all possible parameter
%combinations included)
%   Runs model with input value of parameters
%   Returns map, negllh, and llh, depending on which fitting scheme you are
%   using (maximum likelihood (ML), type II ML, hierarchical Bayesian)

global onesim modeltofit mu sigma realsubjectsflag HBI_flag
%global fit_epsilon_opt

if ~realsubjectsflag
% simulated data, not real
% simulated data is already z-scored for analysis & so doesn't need to be
% z-scored
% simdata = [simdata; subj task rating torate updates misses mains matches];
    subj = unique(onesim(:,1));
    stimuli = onesim(:,2);
    realratings = onesim(:,3);
    display = onesim(:,4);
    nupdates = onesim(:,5);
    nmisses = onesim(:,6);
    mains = onesim(:,7);
    nmatches = onesim(:,8);
    noisiness = onesim(:,9);
    responses = onesim(:,10);
    nlures = onesim(:,11);
    nerrors = onesim(:,12);
    nFAs = onesim(:,13);
elseif realsubjectsflag %fitting real subject data
    % the cost components (maintenance etc.) need to be z-scored to be
    % normally distributed
    subj = unique(onesim.subj);
    stimuli = onesim.task;
    realratings = onesim.BDM;
    display = onesim.display;
    nupdates = zscore(onesim.nupdates); 
    nmisses = zscore(onesim.nmisses);
    mains = zscore(onesim.maintained); 
    nmatches = zscore(onesim.nmatches);
    noisiness = zscore(onesim.noisiness);
    responses = zscore(onesim.nresponses);
    nlures = zscore(onesim.nlures);
    nerrors = zscore(onesim.nerrors);
    nFAs = zscore(onesim.nFAs);
end

% Transform parameters if HBI model fitting being performed
% The CBM package needs parameters to be normally distributed,
% so not bounded at 0 and 1, for example.
if HBI_flag
    params = applyTrans_parameters(modeltofit,params);
end

[uc,epsilon,init,missc,mainc,matchc,noisec,respc,lurec,errorc,fac,alpha,delta] = set_param_values(params,modeltofit);
ntrials = sum(~isnan(realratings));
% in set_param_values, I scale cost parameters up, make sure epislon &
% alpha are always positive, etc. as a function, it makes
% simulating/fitting much easier to do

costs = [uc missc mainc matchc noisec respc lurec errorc fac];
% put all costs in one vector
% majority of models include only a few of the above costs, so the costs
% not in play are set to 0 as a default
% this vector gets fed into the set_new_costs function, but only if the
% update rule for the model is the delta update rule (which adjusts costs
% themselves, as opposed to the alpha learning rule, which is implemented
% through the incremental updating of unchanging cost values)

components = [nupdates nmisses mains nmatches noisiness responses nlures nerrors nFAs];
%costs = [uc missc mainc matchc noisec respc lurec errorc fac];
% these HAVE to be in the right order (i.e. matched up) to draw any
% inferences from the cost parameter values, so be careful changing these!!

%simulate the model for one subject at a time
ratings = [1 init].*(ones(1,max(display)));
% initialize ratings using the different init parameters for each task
% or using just one init parameter, the same for all tasks (this is
% specified in model creation via. coc_createModels.m)

ratings_list = NaN(ntrials,1); %init for each subject 
% initialize to correct length w placeholder values

costs = repmat(costs,ntrials,1);
if (modeltofit.delta || modeltofit.deltai)
    for trial = 2:ntrials
        costs(trial,:) = set_new_costs(costs(1,:),delta,trial);
    end
end
% pre-update the costs according to the delta update rule specified in
% set_new_costs.m, but only if it's a delta-type model. if not, the costs
% are stable.

cost = sum(costs.*components(1:ntrials,:),2); %add all the costs together
for trial = 1:ntrials
    % loop over ratings (32 in total, though 1 subject's data wasn't saved
    % all the way through, so they have 30 or 31 ratings instead)
    
    torate = display(trial);
    % identify which task was up for a rating (or, which fractal was on
    % screen)
    
    if ~isnan(torate) %skip those trials on which data were not saved
        rating = ratings(torate); ratings_list(trial) = rating; %ratings(stim)
    end %end of disqualifying data type for displayed task
    % obtain simulated rating for the task that was completed, to compare to the
    % rating generated by the simulated data/real subject
    
    stim = stimuli(trial);
    % now, which task was actually completed? (won't always be the same as what
    % was rated)
    
    % learn about the cumulative cost of completing that task
    % either with a learning rate of alpha or with a learning rate of 1
    % (complete update every trial)
    if modeltofit.alpha
        ratings(stim) = ratings(stim) + alpha.*(cost(trial,:)-ratings(stim)); %delta rule
    else %alpha fixed or no alpha
        %ratings(stim) = ratings(stim) + alpha*(cost); %compounding cost model
        %ratings(stim) = cost(trial); %no learning, no compounding, no delta rule. just a basic regression on last round
        ratings(stim) = ratings(stim)+ 1*(cost(trial,:)-ratings(stim));
    end
    
end

% % OPTIONAL % %  
% %calculate epsilon optimally
% epsilon_opt = sqrt((1/ntrials) * sum(realratings(~isnan(realratings)) - ratings_list).^2);
% fit_epsilon_opt(subj) = epsilon_opt; %track fit epsilons
% if ~model.epsilon; epsilon = epsilon_opt; end 

% I was doing this previously when I was having trouble recovering the
% correct epsilon values, to try and see whether the true epsilon value
% (which varies here & there because of stochasticity in data simulation)
% matched up more or less with the epsilon value I simulated with.


probs = normpdf(realratings(~isnan(realratings)),ratings_list,epsilon); 
% compare real ratings to the ratings these parameter values produced. are
% they likely?
probs(probs==0) = 1e-100; %can't be 0 exactly for llh

llh = nansum(log(probs));
negllh = -llh;
% obtain llh & negative llh
map = llh;

if ~HBI_flag %run these calculations if running EM_fit
    priors = normpdf(params',mu',sigma');
    lprior = sum(log(priors));
    map = negllh-lprior;
    % incorporate prior over parameter values to the "goodness of fit" of
    % each set of parameters
    % parameters which deviate a lot from the mean parameter values of the
    % group will be penalized
end

end


